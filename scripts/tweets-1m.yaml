data_dir: &DIR ../data/tweets-1m/

# preprocessing
raw_data_file: /shared/data/czhang82/source/tweets-www/clean/tweets.txt
grid_list: [100, 100]
train_ratio: 0.8
valid_ratio: 0.1
min_token_freq: 50

# training
train_data_file: !join [*DIR, input/train.txt]
test_data_file: !join [*DIR, input/test.txt]
valid_data_file: !join [*DIR, input/test.txt]
x_vocab_file: !join [*DIR, input/words.txt]
y_vocab_file: !join [*DIR, input/locations.txt]
train_log_file: !join [*DIR, output/train_log.txt]
performance_file:  !join [*DIR, output/performance.txt]
model_path: !join [*DIR, model/]
case_seed_file: !join [*DIR, input/case_seeds.txt]
case_output_file: !join [*DIR, output/case_outputs.txt]

error_analysis_path: !join [*DIR, output/]
error_instance_file: !join [*DIR, output/instance-]
same_sense_file: !join [*DIR, output/same_sense.txt]

classify_train_file: !join [*DIR, input/classify_train.txt]
classify_test_file: !join [*DIR, input/classify_test.txt]
classify_cat_file: !join [*DIR, input/classify_category.txt]
classify_train_ratio: 0.8

load_pretrained: False
load_model: False
save_model: True
data_worker: 8
n_sense: 2
embedding_dim: 300
batch_size: 1024
learning_rate: 0.005
n_epoch: 20
print_gap: 100
K: 10

eval_dim: True
dim_list: [300, 400, 50]

eval_batch: False
batch_list: [2048, 4096]

eval_lr: False
lr_list: [0.001, 0.002, 0.003, 0.004, 0.01]

eval_sense: False
n_sense_list: [3,4,5]

# model_type_list: ['recon', 'attn', 'sense', 'attn_sense', 'comp_attn_sense', 'bilinear_sense']

model_type_list: ['recon', 'sense', 'comp_attn_sense']
# model_type_list: ['sense']
cmp_model_type_list: ['recon', 'sense']
